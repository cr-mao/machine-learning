## é€»è¾‘å›å½’

é€»è¾‘å›å½’æ˜¯è§£å†³äºŒåˆ†ç±»é—®é¢˜çš„åˆ©å™¨


å›å½’é—®é¢˜æ€ä¹ˆè§£å†³åˆ†ç±»é—®é¢˜ï¼Ÿ

å°†æ ·æœ¬çš„ç‰¹å¾å’Œæ ·æœ¬å‘ç”Ÿçš„æ¦‚ç‡è”ç³»èµ·æ¥ï¼Œæ¦‚ç‡æ˜¯ä¸€ä¸ªæ•°ã€‚

```text
y^ = f(x)

p^ = f(x)

y^ = {
   1,   p^ >=0.5
   0,   p^ <0.5
}
```




### åº”ç”¨åœºæ™¯
- ç–¾ç—…æ˜¯å¦æ˜¯é˜³æ€§
- é“¶è¡Œå¡æˆ¿è´·æ¬¾æ˜¯å¦æ”¾è´·
- é¢„æµ‹å¹¿å‘Šç‚¹å‡»ç‡ï¼ˆæ˜¯å¦ç‚¹å‡»ï¼Œæ˜¯å¦æ¨èè¿™ä¸ªå¹¿å‘Š)
- æ˜¯å¦æ˜¯åƒåœ¾é‚®ä»¶
- æ¨èç³»ç»Ÿä¸­ç”¨åˆ°å¾ˆå¤šäºŒåˆ†ç±»ä»»åŠ¡


### æå¤§ä¼¼ç„¶ä¼°è®¡

æ ¸å¿ƒæ€æƒ³ï¼š

è®¾æ¨¡å‹ä¸­å«æœ‰å¾…ä¼°å‚æ•°wï¼Œå¯ä»¥å–å¾ˆå¤šå€¼ã€‚å·²ç»çŸ¥é“äº†æ ·æœ¬è§‚æµ‹å€¼ï¼Œä»wçš„ä¸€åˆ‡å¯èƒ½å€¼ä¸­ï¼ˆé€‰å‡ºä¸€ä¸ªä½¿è¯¥è§‚å¯Ÿå€¼å‡ºç°çš„æ¦‚ç‡ä¸ºæœ€å¤§çš„å€¼ï¼Œä½œä¸ºwå‚æ•°çš„ä¼°è®¡å€¼ï¼Œè¿™å°±æ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚ï¼ˆé¡¾åæ€ä¹‰ï¼šå°±æ˜¯çœ‹ä¸Šå»é‚£ä¸ªæ˜¯æœ€å¤§å¯èƒ½çš„æ„æ€ï¼‰


å‡è®¾æœ‰ä¸€æšä¸å‡åŒ€çš„ç¡¬å¸ï¼Œå‡ºç°æ­£é¢çš„æ¦‚ç‡å’Œåé¢çš„æ¦‚ç‡æ˜¯ä¸åŒçš„ã€‚å‡å®šå‡ºç°æ­£é¢çš„æ¦‚ç‡ä¸ºğœƒï¼Œ æŠ›äº†6æ¬¡å¾—åˆ°å¦‚ä¸‹ç°è±¡ D = {æ­£é¢ï¼Œåé¢ï¼Œåé¢ï¼Œæ­£é¢ï¼Œæ­£é¢ï¼Œæ­£é¢}ã€‚æ¯æ¬¡æŠ•æ·äº‹ä»¶éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ åˆ™æ ¹æ®äº§ç”Ÿçš„ç°è±¡Dï¼Œæ¥ä¼°è®¡å‚æ•°ğœƒæ˜¯å¤šå°‘?

```text
P(D|ğœƒ) = P {æ­£é¢ï¼Œåé¢ï¼Œåé¢ï¼Œæ­£é¢ï¼Œæ­£é¢ï¼Œæ­£é¢}
 = P(æ­£é¢|ğœƒ) P(æ­£é¢|ğœƒ) P(æ­£é¢|ğœƒ) P(æ­£é¢|ğœƒ) P(æ­£é¢|ğœƒ) P(æ­£é¢|ğœƒ)

=ğœƒ *(1-ğœƒ)*(1-ğœƒ)ğœƒ*ğœƒ*ğœƒ = ğœƒ^4(1 âˆ’ ğœƒ)^2

æ±‚æ­¤å‡½æ•°çš„æå¤§å€¼æ—¶ï¼Œä¼°è®¡ğœƒä¸ºå¤šå°‘
```
```text
å¯¹ä¸Šé¢å‡½æ•°æ±‚å¯¼
4ğœƒ^3.(1-ğœƒ)^2+ ğœƒ^4. 2(1-ğœƒ)*-1
= 4ğœƒ^3.(1-ğœƒ)^2 - 2ğœƒ^4(1-ğœƒ)

= ğœƒ^3.(1-ğœƒ)( 4-4ğœƒ ) - ğœƒ^3.(1-ğœƒ)(2ğœƒ)
= ğœƒ^3.(1-ğœƒ)(4-6ğœƒ ) = 0 

ğœƒ1=0 ,ğœƒ2=1,ğœƒ3=2/3 
å› ä¸º0ï¼Œ1ä¸å¯èƒ½ï¼Œæ‰€ä»¥ğœƒ å–2/3
```

![](images/ml_31.png)

```text
ln(1/a)=ln(a^-1)=-lna
ln(1/2Ï€^(n/2)) = - ln 2Ï€^(n/2) =-n/2 ln 2Ï€
```

### é€»è¾‘å›å½’çš„åŸç†


 åŸºæœ¬æ€æƒ³

1. åˆ©ç”¨çº¿æ€§æ¨¡å‹ f(x) = wx + b æ ¹æ®ç‰¹å¾çš„é‡è¦æ€§è®¡ç®—å‡ºä¸€ä¸ªå€¼
2. å†ä½¿ç”¨ sigmoid å‡½æ•°å°† f(x) çš„è¾“å‡ºå€¼æ˜ å°„ä¸ºæ¦‚ç‡å€¼
   1. è®¾ç½®é˜ˆå€¼(eg:0.5)ï¼Œè¾“å‡ºæ¦‚ç‡å€¼å¤§äº 0.5ï¼Œåˆ™å°†æœªçŸ¥æ ·æœ¬è¾“å‡ºä¸º 1 ç±»
   2. å¦åˆ™è¾“å‡ºä¸º 0 ç±»

3. é€»è¾‘å›å½’çš„å‡è®¾å‡½æ•°
   -  h(w) = sigmoid(wx + b )
   - çº¿æ€§å›å½’çš„è¾“å‡ºï¼Œä½œä¸ºé€»è¾‘å›å½’çš„è¾“å…¥


é€»è¾‘å›å½’ä¸­ï¼Œå…¶è¾“å…¥å€¼æ˜¯ä»€ä¹ˆ
  - é€»è¾‘å›å½’çš„è¾“å…¥å°±æ˜¯ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹
  - h(w) = w1x1 + w2x2 + .... + b 

å¦‚ä½•åˆ¤æ–­é€»è¾‘å›å½’çš„è¾“å‡º
  - sigmoidå‡½æ•°
   ![](images/ml_27.png)

Ïƒ(t) = 1/(1+e^-t)


åˆ¤æ–­æ ‡å‡†

å›å½’çš„ç»“æœè¾“å…¥åˆ°sigmoidå‡½æ•°å½“ä¸­
è¾“å‡ºç»“æœï¼š[0, 1]åŒºé—´ä¸­çš„ä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œé»˜è®¤ä¸º0.5ä¸ºé˜ˆå€¼

![](images/ml_28.png)
![](images/ml_49.png)




sigmodå‡½æ•°å¯å¯¼ï¼Œæ˜¯å•è°ƒé€’å¢å‡½æ•°ã€‚ 

å¯¼å‡½æ•°å…¬å¼ï¼š f'(x) = f(x) (1-f(x))

é€»è¾‘å›å½’æœ€ç»ˆçš„åˆ†ç±»æ˜¯é€šè¿‡å±äºæŸä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼æ¥åˆ¤æ–­æ˜¯å¦å±äºæŸä¸ªç±»åˆ«ï¼Œå¹¶ä¸”è¿™ä¸ªç±»åˆ«é»˜è®¤æ ‡è®°ä¸º1(æ­£ä¾‹),å¦å¤–çš„ä¸€ä¸ªç±»åˆ«ä¼šæ ‡è®°ä¸º0(åä¾‹)ã€‚ï¼ˆæ–¹ä¾¿æŸå¤±è®¡ç®—ï¼‰


![](images/ml_32.png)




### æŸå¤±å‡½æ•°


```text
cost = {
    å¦‚æœy=1, p è¶Šå°ï¼Œcostè¶Šå¤§ï¼Œï¼ˆyæ˜¯çœŸå®å€¼ï¼Œpå°ä»£è¡¨ä¼°è®¡é”™äº†)
    å¦‚æœy=0,pè¶Šå¤§,costè¶Šå¤§
}

cost = {
    -log(p^)   if y = 1
    -log(1-p^) if y=0
}

ä¸€ä¸ªæ ·æœ¬çš„æŸå¤±ï¼š
cost = -ylog(p^) -(1-y) log(1-p^)
```


å…¶æŸå¤±å‡½æ•°é€šå¸¸æ˜¯å¯¹æ•°æŸå¤±å‡½æ•°ï¼ˆlog lossï¼‰ï¼Œä¹Ÿç§°ä¸ºäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆcross-entropy lossï¼‰ã€‚å¯¹äºé€»è¾‘å›å½’æ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°çš„å®šä¹‰å¦‚ä¸‹ï¼š

![](images/ml_34.png)

1. ä¸€ä¸ªæ ·æœ¬
- å‡è®¾æœ‰2ä¸ªç±»åˆ«ï¼Œ1çš„ç±»åˆ«æ¦‚ç‡æ˜¯pï¼Œ 0çš„ç±»åˆ«æ¦‚ç‡æ˜¯1-p
```text
L = {
   p    if y = 1
   1-p  if y = 0
}
æ ·æœ¬æ˜¯1çš„æ¦‚ç‡æ˜¯p,æ ·æœ¬æ˜¯0çš„æ¦‚ç‡æ˜¯1-p

ä¸Šé¢ç­‰ä»·äºè¿™ä¸ªå…¬å¼
L = p^y . (1-p)^(1-y)
```

2. nä¸ªæ ·æœ¬
```text
L = (p1^y1 . (1-p1)^(1-y1)) * (p2^y2 . (1-p2)^(1-y2)) * .... * (p2^yn . (1-p2)^(1-yn)) 
pi ä¸º æ¯ä¸ªæ ·æœ¬ è¢«åˆ†ç±»æ­£ç¡®æ—¶çš„æ¦‚ç‡
yi ä¸ºè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«
```

3. å¯¹ä¸Šé¢å…¬å¼æ±‚å¯¹æ•°è½¬æ¢å…¬å¼
![](images/ml_35.png)

**è®©è”åˆæ¦‚ç‡æœ€å¤§æ—¶ï¼Œä¼°è®¡w,bçš„å‚æ•°ï¼Œå°±æ˜¯æå¤§æ‹Ÿç„¶ä¼°è®¡** 

æœ€å¤§åŒ–é—®é¢˜å˜æˆæœ€å°åŒ–

![](images/ml_36.png)

4. ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ›´æ–°é€»è¾‘å›å½’ç®—æ³•çš„æƒé‡å‚æ•°

æ²¡æœ‰å…¬å¼è§£
![](images/ml_50.png)

è¿™é‡Œçœç•¥næ­¥ï¼Œæœ‰ç‚¹éº»çƒ¦.... 

![](images/ml_51.png)

### å®ç°é€»è¾‘å›å½’ç®—æ³•
```python
import numpy as np
from .metrics import accuracy_score

class LogisticRegression:

    def __init__(self):
        """åˆå§‹åŒ–Logistic Regressionæ¨¡å‹"""
        self.coef_ = None
        self.intercept_ = None
        self._theta = None

    def _sigmoid(self, t):
        return 1. / (1. + np.exp(-t))

    def fit(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """æ ¹æ®è®­ç»ƒæ•°æ®é›†X_train, y_train, ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è®­ç»ƒLogistic Regressionæ¨¡å‹"""
        assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

        # æŸå¤±å€¼
        def J(theta, X_b, y):
            y_hat = self._sigmoid(X_b.dot(theta))
            try:
                return - np.sum(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) / len(y)
            except:
                return float('inf')

        # æ¢¯åº¦
        def dJ(theta, X_b, y):
            return X_b.T.dot(self._sigmoid(X_b.dot(theta)) - y) / len(y)

        # æ¢¯åº¦ä¸‹é™æ³•
        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):

            theta = initial_theta
            cur_iter = 0

            while cur_iter < n_iters:
                gradient = dJ(theta, X_b, y)
                last_theta = theta
                theta = theta - eta * gradient
                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                    break

                cur_iter += 1

            return theta

        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
        initial_theta = np.zeros(X_b.shape[1])
        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

        self.intercept_ = self._theta[0]
        self.coef_ = self._theta[1:]

        return self

    # sigmod å¾—å‡ºçš„æ¦‚ç‡
    def predict_proba(self, X_predict):
        """ç»™å®šå¾…é¢„æµ‹æ•°æ®é›†X_predictï¼Œè¿”å›è¡¨ç¤ºX_predictçš„ç»“æœæ¦‚ç‡å‘é‡"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])
        return self._sigmoid(X_b.dot(self._theta))
    # é¢„æµ‹æ˜¯0è¿˜æ˜¯1 ï¼ŒäºŒåˆ†ç±»
    def predict(self, X_predict):
        """ç»™å®šå¾…é¢„æµ‹æ•°æ®é›†X_predictï¼Œè¿”å›è¡¨ç¤ºX_predictçš„ç»“æœå‘é‡"""
        assert self.intercept_ is not None and self.coef_ is not None, \
            "must fit before predict!"
        assert X_predict.shape[1] == len(self.coef_), \
            "the feature number of X_predict must be equal to X_train"

        proba = self.predict_proba(X_predict)
        # è¿™é‡Œå…ˆå®šæ­» 0ã€‚5 å°±æ˜¯1.
        return np.array(proba >= 0.5, dtype='int')

    def score(self, X_test, y_test):
        """æ ¹æ®æµ‹è¯•æ•°æ®é›† X_test å’Œ y_test ç¡®å®šå½“å‰æ¨¡å‹çš„å‡†ç¡®åº¦"""

        y_predict = self.predict(X_test)
        # è¿™é‡Œæ˜¯ å‡†ç¡®ç‡
        return accuracy_score(y_test, y_predict)

    def __repr__(self):
        return "LogisticRegression()"


```